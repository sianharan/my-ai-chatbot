import streamlit as st
import google.generativeai as genai
import pandas as pd
import os

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="êµìœ¡ ì •ì±… ë¶„ì„ ì „ë¬¸ê°€", layout="wide")
st.title("ğŸ¤– êµìœ¡ ì •ì±… ë¶„ì„ ì „ë¬¸ê°€ ì±—ë´‡")
st.markdown("---")

# 2. API ì„¤ì • ë° ëª¨ë¸ ìë™ íƒìƒ‰ (404 ì˜¤ë¥˜ ë°©ì§€ ë¡œì§)
if "GEMINI_API_KEY" in st.secrets:
    api_key = st.secrets["GEMINI_API_KEY"]
    
    # [í•µì‹¬] transport='rest'ë¥¼ ì„¤ì •í•˜ì—¬ v1beta ê²½ë¡œ ë¬¸ì œë¥¼ ë¬¼ë¦¬ì ìœ¼ë¡œ ìš°íšŒ
    genai.configure(api_key=api_key, transport='rest')
    
    try:
        # í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ìë™ íƒìƒ‰
        available_models = []
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                available_models.append(m.name)
        
        if not available_models:
            st.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. API í‚¤ ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”.")
            st.stop()
            
        # ìµœì ì˜ ëª¨ë¸ ìë™ ì„ íƒ (1.5-flash ì„ í˜¸)
        selected_model_name = next((m for m in available_models if 'gemini-1.5-flash' in m), available_models[0])
        model = genai.GenerativeModel(selected_model_name)
        
        st.success(f"âœ… ì‹œìŠ¤í…œ ì—°ê²° ì™„ë£Œ: `{selected_model_name}`")
        
    except Exception as e:
        st.error(f"âš ï¸ ëª¨ë¸ íƒìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.stop()
else:
    st.error("âš ï¸ Streamlit Secretsì— 'GEMINI_API_KEY'ë¥¼ ë“±ë¡í•´ ì£¼ì„¸ìš”!")
    st.stop()

# 3. ë°ì´í„° ë¡œë“œ (ìºì‹± ì ìš©)
@st.cache_data
def load_policy_data(file_name):
    if not os.path.exists(file_name):
        return None, f"'{file_name}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    try:
        # ì—‘ì…€ ì—”ì§„ ëª…ì‹œí•˜ì—¬ ì•ˆì •ì„± í™•ë³´
        df = pd.read_excel(file_name, engine='openpyxl')
        text_content = ""
        for i, row in df.iterrows():
            title = str(row.get('ì œëª©', 'ì œëª© ì—†ìŒ'))
            content = str(row.get('ë‚´ìš©', 'ë‚´ìš© ì—†ìŒ'))
            text_content += f"[{i+1}ë²ˆ ì œì•ˆ] ì œëª©: {title} / ë‚´ìš©: {content}\n\n"
        return text_content, None
    except Exception as e:
        return None, f"ë°ì´í„° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# íŒŒì¼ ë¡œë“œ (íŒŒì¼ëª…ì´ ì •í™•í•œì§€ ë‹¤ì‹œ í•œ ë²ˆ í™•ì¸í•˜ì„¸ìš”)
policy_text, error_msg = load_policy_data("ì •ì±…ì œì•ˆ_6ê°œì›”.xlsx")

if error_msg:
    st.error(error_msg)
    st.stop()

# 4. ì±„íŒ… ì‹œìŠ¤í…œ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state.messages = []

# ê¸°ì¡´ ëŒ€í™” í‘œì‹œ
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# 5. ì§ˆë¬¸ ì²˜ë¦¬ ë° AI ì‘ë‹µ (ì§„í–‰ ìƒíƒœ í‘œì‹œ ì¶”ê°€)
if prompt := st.chat_input("ì •ì±…ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•´ ë³´ì„¸ìš”."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥ ë° í‘œì‹œ
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    try:
        with st.chat_message("assistant"):
            # ë¡œë”© ìŠ¤í”¼ë„ˆë¡œ ì§„í–‰ ìƒí™© ì•Œë¦¼
            with st.spinner("ğŸ¤– AIê°€ ì •ì±… ë°ì´í„°ë¥¼ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                full_prompt = f"""ë‹¹ì‹ ì€ êµìœ¡ ì •ì±… ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì•„ë˜ ì œê³µëœ [ë°ì´í„°]ë§Œì„ ê·¼ê±°ë¡œ ë‹µë³€í•˜ì„¸ìš”. 
ë‹µë³€ ì‹œ ê´€ë ¨ëœ ì œì•ˆì˜ ë²ˆí˜¸(ì˜ˆ: [1ë²ˆ ì œì•ˆ])ë¥¼ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”.

[ë°ì´í„°]
{policy_text}

[ì§ˆë¬¸]
{prompt}"""
                
                # ë‹µë³€ ìƒì„±
                response = model.generate_content(full_prompt)
                
                if response and response.text:
                    st.markdown(response.text)
                    st.session_state.messages.append({"role": "assistant", "content": response.text})
                else:
                    st.error("AI ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.")
                    
    except Exception as e:
        st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
