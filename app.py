import streamlit as st
import google.generativeai as genai
import pandas as pd
import os

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="êµìœ¡ ì •ì±… ë¶„ì„ ì „ë¬¸ê°€", layout="wide")
st.title("ğŸ¤– êµìœ¡ ì •ì±… ë¶„ì„ ì „ë¬¸ê°€ ì±—ë´‡")
st.info("ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ì±… ì œì•ˆì„ ì •ë°€ ë¶„ì„í•©ë‹ˆë‹¤.")

# 2. API ì„¤ì • ë° ì£¼ì†Œ ê°•ì œ ê³ ì •
if "GEMINI_API_KEY" in st.secrets:
    api_key = st.secrets["GEMINI_API_KEY"]
    
    # [í•µì‹¬] v1beta ì£¼ì†Œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ v1 ì •ì‹ ì£¼ì†Œë¥¼ ê°•ì œë¡œ ì‚¬ìš©í•˜ê²Œ í•©ë‹ˆë‹¤.
    # transport='rest' ì„¤ì •ì€ API í†µì‹  ê·œê²©ì„ ê°€ì¥ ì•ˆì •ì ì¸ ë°©ì‹ìœ¼ë¡œ ê³ ì •í•©ë‹ˆë‹¤.
    genai.configure(api_key=api_key, transport='rest')
    
    try:
        # ëª¨ë¸ ê°ì²´ ìƒì„± (ê²½ë¡œ ì—†ì´ ì´ë¦„ë§Œ ì‚¬ìš©)
        model = genai.GenerativeModel('gemini-1.5-flash')
    except Exception as e:
        st.error(f"ëª¨ë¸ ì„¤ì • ì˜¤ë¥˜: {e}")
else:
    st.error("âš ï¸ Secretsì— 'GEMINI_API_KEY'ë¥¼ ë“±ë¡í•´ ì£¼ì„¸ìš”!")
    st.stop()

# 3. ë°ì´í„° ë¡œë“œ (ìºì‹±)
@st.cache_data
def load_data(file_name):
    if not os.path.exists(file_name):
        return None, f"íŒŒì¼({file_name})ì´ ì—†ìŠµë‹ˆë‹¤. ì—…ë¡œë“œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”."
    try:
        df = pd.read_excel(file_name)
        text = ""
        for i, row in df.iterrows():
            t = str(row.get('ì œëª©', 'ì œëª©ì—†ìŒ'))
            c = str(row.get('ë‚´ìš©', 'ë‚´ìš©ì—†ìŒ'))
            text += f"[{i+1}ë²ˆ ì œì•ˆ] ì œëª©: {t} / ë‚´ìš©: {c}\n\n"
        return text, None
    except Exception as e:
        return None, f"ì—‘ì…€ ì½ê¸° ì˜¤ë¥˜: {e}"

policy_text, error = load_data("ì •ì±…ì œì•ˆ_6ê°œì›”.xlsx")
if error:
    st.error(error)
    st.stop()

# 4. ì±„íŒ… ì‹œìŠ¤í…œ
if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])

# 5. ì§ˆë¬¸ ì²˜ë¦¬ ë° AI ì‘ë‹µ
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    try:
        with st.chat_message("assistant"):
            # ì „ë¬¸ê°€ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            full_prompt = f"ë°ì´í„°ë¥¼ ì°¸ê³ í•´ ë‹µí•´ì¤˜.\n[ë°ì´í„°]\n{policy_text}\n[ì§ˆë¬¸]\n{prompt}"
            
            # API ë²„ì „ì„ ê°•ì œë¡œ ì¸ì‹ì‹œí‚¤ê¸° ìœ„í•´ 
            # ë‚´ë¶€ì ì¸ ì£¼ì†Œ ì²´ê³„ ë¬¸ì œë¥¼ ìš°íšŒí•˜ëŠ” í˜¸ì¶œ ë°©ì‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
            response = model.generate_content(full_prompt)
            
            if response and response.text:
                st.markdown(response.text)
                st.session_state.messages.append({"role": "assistant", "content": response.text})
            else:
                st.error("AI ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•˜ì„¸ìš”.")
                
    except Exception as e:
        # ë§Œì•½ ì—¬ì „íˆ v1beta ë©”ì‹œì§€ê°€ ëœ¬ë‹¤ë©´, ì„œë²„ì— êµ¬í˜• ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ê¹”ë ¤ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.
        st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        st.warning("ë„ì›€ë§: GitHubì˜ requirements.txt íŒŒì¼ì— google-generativeai==0.8.3 ê°€ ìˆëŠ”ì§€ ê¼­ í™•ì¸í•˜ì„¸ìš”.")
